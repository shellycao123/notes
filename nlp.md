## Introduction
1. named entity tagging: mark name types
2. chunking:  mark verb groups/ noun groups. approximate syntactic groups. 
3. high levle parsing is O(N^3)
4. Wordnet: a manually created resource for words with multiple meanings. 
5. Word Sense Structure:
   * Predicate/ Argument Structure: characterize predictable paraphrases. Use semantic role labeling. 
6. Anaphora:
    * coreference: multiple worlds refer to the same object.
    * Type coref: refer to different object, but the same type of object. 
    * Bridging Anaphora: multiple nouns are connected.
7. Discourse Argument Structure: relationship of nouns between sentences. adv, conj, link clauses. (however, although, etc. )
8. Manual Annotation: used to create, test and fine-tune task. 
   - measure of agreement: Kappa
   - Hope annotators agree on classfication most of the time; machine is unlikely to perform tasks human cannot agree upon. 
   - used to create answer keys to score system output. 
     - precision = correct answers/ length of the system output
     - recall = coorect answers/length of answer key
   - Divide corpus into sub-corpora
     - training corpus: acquire statistical patterns
     - test corpus: measure system performance after development. Should be used infrequently to avoid bias. 
     - development corpus: similar to test corpus, tune system to get better results. 
    - can train system on a specific genre or a diversed set. 
9. Mae tool: annotation tool

## Lesson 2
1. Formal language: can model a phenomenon. 
2. formal grammar: set of rules matches **all and only** sentences in the language. 
 - defines a formal language. 
 - used to generate & recognize language. 
 - parsing: recognize the string & analysis
 - compiler: translate language A to B
 - consisted of: 
   - N: finite set of nonterminal symbols
   - T: finit set of terminal symbols
   - R: set of rewrite rules. 
   - S: special non terminal node that is the start symbol.
   - Chmosky hierarchy: Type 0<= Type 1 <= Type 2 <= Type 3
     - Type 0: no restrictions on the rules, a Turing machine. exponential
     - Type 1: Context-sensitive rules:left-hand sides and right-hand sides of any production rules may be surrounded by a context of terminal and nonterminal symbols. run in polynomial
     - Type 2: context-free rules: every production rule is of the form where left is a single nonterminal symbol, and right is a string of terminals and/or nonterminals.
       - used by parsers & model syntatic structure. run in O(n^3)
     - Type 3: regular  (finite state) grammars. 
       - right regular: all nonterminals are to the right of the terminals on the right side of the rule
       - left regualar: all nonterminals are to the left of the terminals on the right side of the rule
       - processed in O(nlogn). used by chunkers. includes regular expression & finite state automata.
     - Type 0 most expressive, least efficient; Type 3 most efficient, least expressive
3. Regular expression: formular to specify a set of strings
   - generated by concatnation, disjunction(| or [disjunct characters]), repetition(* or {# of repetations}) of regex. 
   - ^ in bracket is for negation, ? for optionality, + for 1 or more, . for any character, ^ not in bracket beginning of line, $ end of line
4. grep: search in a text in linux. 
   - grep -E: use with regular expression. 
5. Finite State Automata: device to recognize finite state grammars. 
   - Deterministic: finite state automata(DFSA): rules are unambiguous, only one definition.
   - Undeterministic: rules are ambiguous, nultiple possible next stops. can have multiple definitions. 
    - can be converted into a bigger DFSA machine. 

## Part of Speech Tagging
1. POS tagsets: Assume Particular Tokenizations & distinguish inflections & different instance of the same string can have different tags
2. Annotators & POS taggers assign tags to each token in a sentence.
   - POS taggers assign 1 POS tag to each input token.
   - different tokenizer can result in diffferent tagging. 
3. Most POS taggers assume sentence divisions.
   - Pipeline:  sentence splitting -> tokenization -> part of speech tagging -> chunking -> parsing
   - errors in the earlier stages in the pipeline will be amplified. Error propogates. 
   - multiple sentences within quotes are assumed separate. 
4. method: assign potential POS tags to words based on dictionary then manual rules. 
  - use probability as well: assume future evens based on past observations. 
   P(event) = num of events/num of trials
   Conditional Porbability P(X|Y) = P(X, Y)/P(Y)
  - find the most probable corresponding tag of word. 
  - Commonly used training corpus: Penn Treebank II. 
  - If a sentence is not in he training corpus, the probability is 0.
  - Find the sequence of tags such that it maximize products of P(w_i | t_i)* P(t_i| t_i-1)
5. Weighted Finite state automaton(WFSA)
   - Edges between nodes represent the possibilty of the transition.
   - Markov chain is a WFSA that an input string uniquely determine paths through the Automaton. 
6. Viterbi Algorithm: remembers the start and end state!
7. Unknown words(OOV): words did not appear in the corpus.
   - can assign arbitrary possibility
   - can use some rules to make educated guesses 
   - can use probability of low frequency words as probability of OOV.
   - can use a combination of above. 

## Information Retrieval & related applications
1. Documents: represented by vectors. unit can be words, N-grams, chunks.
   - vetor can measure similarity of 2 documents.
   - each vector has a score/weight of a unit.  
2. Ad-hoc information retrieval: a collection of documents and a query. 
   - Terms and documents are set of terms.
   - find documents closest to query.
   - component of web search. 
3. TFIDF: Common Weight for Vector. Determine term weights. 
   - Term Frequency( # of term appearance in the document) * Inverse document frequency: log(totalDocument/# of documents containing the term)
   - high score for term characteristic of a document/query, high for infrequent terms. 
   - IDF is normalized by taking the log(reduce the speed of growth)
4. Cosine similarity: high cosine for smaller angle. Determine the similarity of documents. 
5. List of stop words: words not considered to be in the vector
6. stemming procedures used to make equivalence classes( different tense of the same word etc). can also identify similar words( synonyms etc)
7. Application: document searchingL 
   - Recall: correct_answer/ all_correct_answers
   - Precision: correct / all_produced_answer. 
   - some parameters can trade between precision and recall. 
   - Mean Average Precision: used to score IR output. compute precison at several intervals and average. each interval has the same increment of recall percentage. 
8. Document classification &  question answering
   - supervised document classification: pregrouped documents, need to assign unknown document to the category.
   - unsupervised document classification: no pre-grouping. group similar documents together. need a stopping point(minimum grouping value/minimum group numbers). no guarantee to be natural classes.
   - topic modeling: reduce the dimension of document vectors. 
   - use accuracy for evaluation: correct /number of documents. (recall & precision are sued when the # of produced results are different from the answer key)
9. Sentiment Analysis: tend to have high error rate. 
    - find certain words & calculate similarity
    - notice negation, words with different attitudes.

## Statistical language model
1. probability distribution over sequence of words( or other lingustic units)
2. use to rank word sequences by likelihood, might produce more than one output. 
3. bigram:  how frequent two words appear together. 
4. Markov assumptions: 
   - Unigram: each word is independent of each other
   - bigram: one word depends on the previous word. 
5. if an n-gram is not found in training corpus, back off to n - 1 gram. 

## Phrase Structure Model
1. Possible sentences in a language are modeled by a set of context-free phrase structure rules. 
2. typically models the distribution of words and sequences. 
3. Tests for measuring "good" phrase structure model: 
   - same category tests: if two phrases have the same phrase lable, A & B should be able to exchanged in syntactically well-formed sentences. two words particiating in th same inflectional paradigm are probably the same part of speech. (e.g like, likes, liking)
   - constituency tests: 
     - a constituent is a word or a group of words that function as a single unit. 
     - units of the same type should be able to conjoin. (e.g A and B), hasa exceptions. units can occur as fragments if answering question. anaphora resolution: units are refered by pronouns. 

4. Chomsky Normal Form (CNF): subtype of context free phrase structure rules. right side has at most 2 non determinals. any context-free grammar can be converted to CNF. 
5. CKY Parser: 
   1. Tokenization: position between tokens are numbered. [1, 3] represent subsequences. 
   2. Chart parser: started with rules. combine the units following the right side of the rule to create the left side. Final: convers the whole string. 
   3. search strategy: how to find the next phrase to combine. 
       -bottom up: start from rules where right side is the basic unit. 
       - top down: start from the rule where left side is S. 
       - breath first: do multiple constituents in parallel. 
       - depth first
6. Head of a phrase: word that determines the category of the phrase. heads typically subcategorize other members in the phrase.  
   - Criteria: 
      1. select other members in the phrase(subcategrization)
         - semantic selection: select based on the meaning of the head. 
         - statistical selection: used in NLP, find object often correlate with the head. 
      2. determine the distribution of phrases. 
      3. determine the agreement properties of phrases (singular/plural, gender)
7. Argument sharing properties of Lexicon: connected by the phrase. 
   - raising: former is the subject/object of latter
   - control: former is subject/object of both the argument and the latter. 



   
 